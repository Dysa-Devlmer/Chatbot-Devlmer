time=2025-11-26T04:17:43.976-03:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Windows\\system32\\config\\systemprofile\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-11-26T04:17:43.999-03:00 level=INFO source=images.go:522 msg="total blobs: 0"
time=2025-11-26T04:17:43.999-03:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-26T04:17:43.999-03:00 level=INFO source=routes.go:1577 msg="Listening on 127.0.0.1:11434 (version 0.12.9)"
time=2025-11-26T04:17:44.000-03:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-26T04:17:44.008-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 54397"
time=2025-11-26T04:17:44.111-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 54404"
time=2025-11-26T04:17:44.336-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 54411"
time=2025-11-26T04:17:44.532-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 54418"
time=2025-11-26T04:17:44.532-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 54419"
time=2025-11-26T04:17:44.778-03:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-dadf7e43-b580-d107-63fb-993daaa7f187 filtered_id="" library=CUDA compute=7.5 name=CUDA0 description="NVIDIA GeForce GTX 1660 SUPER" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:2b:00.0 type=discrete total="6.0 GiB" available="5.0 GiB"
time=2025-11-26T04:17:44.778-03:00 level=INFO source=routes.go:1618 msg="entering low vram mode" "total vram"="6.0 GiB" threshold="20.0 GiB"
time=2025-11-26T04:29:03.247-03:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Windows\\system32\\config\\systemprofile\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-11-26T04:29:03.293-03:00 level=INFO source=images.go:522 msg="total blobs: 0"
time=2025-11-26T04:29:03.294-03:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-26T04:29:03.295-03:00 level=INFO source=routes.go:1577 msg="Listening on 127.0.0.1:11434 (version 0.12.9)"
time=2025-11-26T04:29:03.296-03:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-26T04:29:03.304-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63867"
time=2025-11-26T04:29:03.516-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63874"
time=2025-11-26T04:29:03.609-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63880"
time=2025-11-26T04:29:03.848-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63887"
time=2025-11-26T04:29:03.848-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63886"
time=2025-11-26T04:29:04.053-03:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-dadf7e43-b580-d107-63fb-993daaa7f187 filtered_id="" library=CUDA compute=7.5 name=CUDA0 description="NVIDIA GeForce GTX 1660 SUPER" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:2b:00.0 type=discrete total="6.0 GiB" available="5.0 GiB"
time=2025-11-26T04:29:04.053-03:00 level=INFO source=routes.go:1618 msg="entering low vram mode" "total vram"="6.0 GiB" threshold="20.0 GiB"
time=2025-11-26T21:50:30.826-03:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Windows\\system32\\config\\systemprofile\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-11-26T21:50:30.914-03:00 level=INFO source=images.go:522 msg="total blobs: 0"
time=2025-11-26T21:50:30.915-03:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-26T21:50:30.921-03:00 level=INFO source=routes.go:1577 msg="Listening on 127.0.0.1:11434 (version 0.12.9)"
time=2025-11-26T21:50:30.926-03:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-26T21:50:30.943-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59267"
time=2025-11-26T21:50:32.050-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 52122"
time=2025-11-26T21:50:32.879-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 53131"
time=2025-11-26T21:50:33.183-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62925"
time=2025-11-26T21:50:33.189-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62926"
time=2025-11-26T21:50:33.617-03:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-dadf7e43-b580-d107-63fb-993daaa7f187 filtered_id="" library=CUDA compute=7.5 name=CUDA0 description="NVIDIA GeForce GTX 1660 SUPER" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:2b:00.0 type=discrete total="6.0 GiB" available="5.0 GiB"
time=2025-11-26T21:50:33.617-03:00 level=INFO source=routes.go:1618 msg="entering low vram mode" "total vram"="6.0 GiB" threshold="20.0 GiB"
time=2025-11-26T22:51:13.480-03:00 level=INFO source=download.go:177 msg="downloading f5074b1221da in 16 273 MB part(s)"
time=2025-11-26T22:51:58.932-03:00 level=INFO source=download.go:177 msg="downloading 43070e2d4e53 in 1 11 KB part(s)"
time=2025-11-26T22:52:00.385-03:00 level=INFO source=download.go:177 msg="downloading 1ff5b64b61b9 in 1 799 B part(s)"
time=2025-11-26T22:52:01.923-03:00 level=INFO source=download.go:177 msg="downloading ed11eda7790d in 1 30 B part(s)"
time=2025-11-26T22:52:03.383-03:00 level=INFO source=download.go:177 msg="downloading 1064e17101bd in 1 487 B part(s)"
time=2025-11-26T22:52:17.053-03:00 level=INFO source=download.go:177 msg="downloading 2bada8a74506 in 16 292 MB part(s)"
time=2025-11-26T22:53:06.535-03:00 level=INFO source=download.go:177 msg="downloading 66b9ea09bd5b in 1 68 B part(s)"
time=2025-11-26T22:53:08.320-03:00 level=INFO source=download.go:177 msg="downloading eb4402837c78 in 1 1.5 KB part(s)"
time=2025-11-26T22:53:09.796-03:00 level=INFO source=download.go:177 msg="downloading 832dd9e00a68 in 1 11 KB part(s)"
time=2025-11-26T22:53:16.138-03:00 level=INFO source=download.go:177 msg="downloading 2f15b3218f05 in 1 487 B part(s)"
time=2025-11-26T22:56:23.865-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 53796"
time=2025-11-26T22:56:24.685-03:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-11-26T22:56:24.685-03:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=8 efficiency=0 threads=16
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from C:\Windows\system32\config\systemprofile\.ollama\models\blobs\sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-7...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7B
llama_model_loader: - kv  12:                               general.tags arr[str,2]       = ["chat", "text-generation"]
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-11-26T22:56:24.931-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Windows\\system32\\config\\systemprofile\\.ollama\\models\\blobs\\sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 --port 53804"
time=2025-11-26T22:56:24.934-03:00 level=INFO source=server.go:470 msg="system memory" total="31.9 GiB" free="10.4 GiB" free_swap="12.4 GiB"
time=2025-11-26T22:56:24.934-03:00 level=INFO source=server.go:522 msg=offload library=CUDA layers.requested=-1 layers.model=29 layers.offload=26 layers.split=[26] memory.available="[5.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.9 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.9 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
time=2025-11-26T22:56:24.971-03:00 level=INFO source=runner.go:910 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\zeNk0\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-haswell.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce GTX 1660 SUPER, compute capability 7.5, VMM: yes, ID: GPU-dadf7e43-b580-d107-63fb-993daaa7f187
The following devices will have suboptimal performance due to a lack of tensor cores:
  Device 0: NVIDIA GeForce GTX 1660 SUPER
Consider compiling with CMAKE_CUDA_ARCHITECTURES=61-virtual;80-virtual and DGGML_CUDA_FORCE_MMQ to force the use of the Pascal code for Turing.
load_backend: loaded CUDA backend from C:\Users\zeNk0\AppData\Local\Programs\Ollama\lib\ollama\cuda_v13\ggml-cuda.dll
time=2025-11-26T22:56:25.212-03:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-11-26T22:56:25.214-03:00 level=INFO source=runner.go:946 msg="Server listening on 127.0.0.1:53804"
time=2025-11-26T22:56:25.227-03:00 level=INFO source=runner.go:845 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:26[ID:GPU-dadf7e43-b580-d107-63fb-993daaa7f187 Layers:26(2..27)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-26T22:56:25.228-03:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-26T22:56:25.229-03:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce GTX 1660 SUPER) (0000:2b:00.0) - 5132 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from C:\Windows\system32\config\systemprofile\.ollama\models\blobs\sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-7...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7B
llama_model_loader: - kv  12:                               general.tags arr[str,2]       = ["chat", "text-generation"]
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 26 repeating layers to GPU
load_tensors: offloaded 26/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   710.78 MiB
load_tensors:        CUDA0 model buffer size =  3457.31 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache:      CUDA0 KV buffer size =   208.00 MiB
llama_kv_cache:        CPU KV buffer size =    16.00 MiB
llama_kv_cache: size =  224.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    17.01 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 32 (with bs=512), 3 (with bs=1)
time=2025-11-26T22:57:08.793-03:00 level=INFO source=server.go:1289 msg="llama runner started in 43.86 seconds"
time=2025-11-26T22:57:08.794-03:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-11-26T22:57:08.794-03:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-26T22:57:08.794-03:00 level=INFO source=server.go:1289 msg="llama runner started in 43.86 seconds"
time=2025-11-26T23:05:09.582-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 61351"
time=2025-11-26T23:05:11.186-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 61362"
time=2025-11-26T23:05:11.437-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 61370"
time=2025-11-26T23:05:11.688-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 61378"
time=2025-11-26T23:05:11.937-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 61386"
time=2025-11-26T23:05:12.187-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 61394"
time=2025-11-26T23:05:12.482-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 50711"
time=2025-11-26T23:05:13.309-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62418"
time=2025-11-26T23:05:13.539-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62427"
time=2025-11-26T23:05:13.747-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62434"
time=2025-11-26T23:05:13.972-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62441"
time=2025-11-26T23:05:14.191-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62448"
time=2025-11-26T23:05:14.438-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62455"
time=2025-11-26T23:05:14.689-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62463"
time=2025-11-26T23:05:14.936-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62470"
time=2025-11-26T23:05:15.186-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62477"
time=2025-11-26T23:05:15.436-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62484"
time=2025-11-26T23:05:15.687-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62492"
time=2025-11-27T03:31:42.053-03:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Windows\\system32\\config\\systemprofile\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-11-27T03:31:42.138-03:00 level=INFO source=images.go:522 msg="total blobs: 10"
time=2025-11-27T03:31:42.139-03:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-27T03:31:42.142-03:00 level=INFO source=routes.go:1577 msg="Listening on 127.0.0.1:11434 (version 0.12.9)"
time=2025-11-27T03:31:42.146-03:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-27T03:31:42.196-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 49675"
time=2025-11-27T03:31:43.377-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 49695"
time=2025-11-27T03:31:44.166-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 49705"
time=2025-11-27T03:31:44.457-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 49717"
time=2025-11-27T03:31:44.458-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 49718"
time=2025-11-27T03:31:45.808-03:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-dadf7e43-b580-d107-63fb-993daaa7f187 filtered_id="" library=CUDA compute=7.5 name=CUDA0 description="NVIDIA GeForce GTX 1660 SUPER" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:2b:00.0 type=discrete total="6.0 GiB" available="5.0 GiB"
time=2025-11-27T03:31:45.808-03:00 level=INFO source=routes.go:1618 msg="entering low vram mode" "total vram"="6.0 GiB" threshold="20.0 GiB"
time=2025-11-27T23:33:24.545-03:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Windows\\system32\\config\\systemprofile\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-11-27T23:33:24.643-03:00 level=INFO source=images.go:522 msg="total blobs: 10"
time=2025-11-27T23:33:24.644-03:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-27T23:33:24.648-03:00 level=INFO source=routes.go:1577 msg="Listening on 127.0.0.1:11434 (version 0.12.9)"
time=2025-11-27T23:33:24.653-03:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-27T23:33:24.831-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59521"
time=2025-11-27T23:33:25.878-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 49857"
time=2025-11-27T23:33:26.693-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 54205"
time=2025-11-27T23:33:26.997-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63910"
time=2025-11-27T23:33:26.997-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63911"
time=2025-11-27T23:33:27.406-03:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-dadf7e43-b580-d107-63fb-993daaa7f187 filtered_id="" library=CUDA compute=7.5 name=CUDA0 description="NVIDIA GeForce GTX 1660 SUPER" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:2b:00.0 type=discrete total="6.0 GiB" available="5.0 GiB"
time=2025-11-27T23:33:27.406-03:00 level=INFO source=routes.go:1618 msg="entering low vram mode" "total vram"="6.0 GiB" threshold="20.0 GiB"
time=2025-11-28T04:42:44.317-03:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Windows\\system32\\config\\systemprofile\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-11-28T04:42:44.346-03:00 level=INFO source=images.go:522 msg="total blobs: 10"
time=2025-11-28T04:42:44.347-03:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-28T04:42:44.348-03:00 level=INFO source=routes.go:1577 msg="Listening on 127.0.0.1:11434 (version 0.12.9)"
time=2025-11-28T04:42:44.350-03:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-28T04:42:44.357-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 52345"
time=2025-11-28T04:42:44.997-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 52354"
time=2025-11-28T04:42:45.002-03:00 level=INFO source=runner.go:498 msg="failure during GPU discovery" OLLAMA_LIBRARY_PATH="[C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v13]" extra_envs=map[] error="failed to finish discovery before timeout"
time=2025-11-28T04:42:45.003-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 52355"
time=2025-11-28T04:42:45.004-03:00 level=INFO source=runner.go:498 msg="failure during GPU discovery" OLLAMA_LIBRARY_PATH="[C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\rocm]" extra_envs=map[] error="failed to finish discovery before timeout"
time=2025-11-28T04:42:45.005-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 52356"
time=2025-11-28T04:42:45.007-03:00 level=INFO source=runner.go:498 msg="failure during GPU discovery" OLLAMA_LIBRARY_PATH="[C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v12]" extra_envs="map[CUDA_VISIBLE_DEVICES:GPU-dadf7e43-b580-d107-63fb-993daaa7f187 GGML_CUDA_INIT:1]" error="failed to finish discovery before timeout"
time=2025-11-28T04:42:45.007-03:00 level=INFO source=types.go:60 msg="inference compute" id=cpu library=cpu compute="" name=cpu description=cpu libdirs=ollama driver="" pci_id="" type="" total="31.9 GiB" available="26.8 GiB"
time=2025-11-28T04:42:45.007-03:00 level=INFO source=routes.go:1618 msg="entering low vram mode" "total vram"="0 B" threshold="20.0 GiB"
time=2025-11-28T23:14:58.080-03:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Windows\\system32\\config\\systemprofile\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-11-28T23:14:58.192-03:00 level=INFO source=images.go:522 msg="total blobs: 10"
time=2025-11-28T23:14:58.193-03:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-28T23:14:58.198-03:00 level=INFO source=routes.go:1577 msg="Listening on 127.0.0.1:11434 (version 0.12.9)"
time=2025-11-28T23:14:58.203-03:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-28T23:14:58.217-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 49676"
time=2025-11-28T23:14:59.263-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 49695"
time=2025-11-28T23:15:00.102-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 49709"
time=2025-11-28T23:15:00.410-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 49721"
time=2025-11-28T23:15:00.412-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 49722"
time=2025-11-28T23:15:01.797-03:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-dadf7e43-b580-d107-63fb-993daaa7f187 filtered_id="" library=CUDA compute=7.5 name=CUDA0 description="NVIDIA GeForce GTX 1660 SUPER" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:2b:00.0 type=discrete total="6.0 GiB" available="5.0 GiB"
time=2025-11-28T23:15:01.797-03:00 level=INFO source=routes.go:1618 msg="entering low vram mode" "total vram"="6.0 GiB" threshold="20.0 GiB"
time=2025-11-29T22:57:32.740-03:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Windows\\system32\\config\\systemprofile\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-11-29T22:57:32.931-03:00 level=INFO source=images.go:522 msg="total blobs: 10"
time=2025-11-29T22:57:32.933-03:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-29T22:57:32.938-03:00 level=INFO source=routes.go:1577 msg="Listening on 127.0.0.1:11434 (version 0.12.9)"
time=2025-11-29T22:57:32.944-03:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-29T22:57:32.955-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60140"
time=2025-11-29T22:57:34.088-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 64517"
time=2025-11-29T22:57:34.949-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 64176"
time=2025-11-29T22:57:35.312-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 65439"
time=2025-11-29T22:57:35.313-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 65440"
time=2025-11-29T22:57:35.708-03:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-dadf7e43-b580-d107-63fb-993daaa7f187 filtered_id="" library=CUDA compute=7.5 name=CUDA0 description="NVIDIA GeForce GTX 1660 SUPER" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:2b:00.0 type=discrete total="6.0 GiB" available="5.0 GiB"
time=2025-11-29T22:57:35.709-03:00 level=INFO source=routes.go:1618 msg="entering low vram mode" "total vram"="6.0 GiB" threshold="20.0 GiB"
time=2025-11-30T22:52:17.817-03:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Windows\\system32\\config\\systemprofile\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-11-30T22:52:17.899-03:00 level=INFO source=images.go:522 msg="total blobs: 10"
time=2025-11-30T22:52:17.901-03:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-30T22:52:17.904-03:00 level=INFO source=routes.go:1577 msg="Listening on 127.0.0.1:11434 (version 0.12.9)"
time=2025-11-30T22:52:17.908-03:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-30T22:52:17.924-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 49686"
time=2025-11-30T22:52:19.256-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59112"
time=2025-11-30T22:52:20.107-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 54601"
time=2025-11-30T22:52:20.593-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 55761"
time=2025-11-30T22:52:20.698-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 55772"
time=2025-11-30T22:52:21.068-03:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-dadf7e43-b580-d107-63fb-993daaa7f187 filtered_id="" library=CUDA compute=7.5 name=CUDA0 description="NVIDIA GeForce GTX 1660 SUPER" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:2b:00.0 type=discrete total="6.0 GiB" available="5.0 GiB"
time=2025-11-30T22:52:21.068-03:00 level=INFO source=routes.go:1618 msg="entering low vram mode" "total vram"="6.0 GiB" threshold="20.0 GiB"
time=2025-12-01T13:39:02.462-03:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Windows\\system32\\config\\systemprofile\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-12-01T13:39:02.637-03:00 level=INFO source=images.go:522 msg="total blobs: 10"
time=2025-12-01T13:39:02.643-03:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-12-01T13:39:02.650-03:00 level=INFO source=routes.go:1577 msg="Listening on 127.0.0.1:11434 (version 0.12.9)"
time=2025-12-01T13:39:02.663-03:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-12-01T13:39:02.681-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 65437"
time=2025-12-01T13:39:03.828-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60434"
time=2025-12-01T13:39:04.714-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63603"
time=2025-12-01T13:39:05.079-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 49688"
time=2025-12-01T13:39:05.079-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 49689"
time=2025-12-01T13:39:05.522-03:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-dadf7e43-b580-d107-63fb-993daaa7f187 filtered_id="" library=CUDA compute=7.5 name=CUDA0 description="NVIDIA GeForce GTX 1660 SUPER" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:2b:00.0 type=discrete total="6.0 GiB" available="5.0 GiB"
time=2025-12-01T13:39:05.523-03:00 level=INFO source=routes.go:1618 msg="entering low vram mode" "total vram"="6.0 GiB" threshold="20.0 GiB"
time=2025-12-01T14:03:40.717-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 64017"
time=2025-12-01T14:03:41.375-03:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-12-01T14:03:41.376-03:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=8 efficiency=0 threads=16
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from C:\Windows\system32\config\systemprofile\.ollama\models\blobs\sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-7...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7B
llama_model_loader: - kv  12:                               general.tags arr[str,2]       = ["chat", "text-generation"]
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-12-01T14:03:41.696-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Windows\\system32\\config\\systemprofile\\.ollama\\models\\blobs\\sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 --port 64029"
time=2025-12-01T14:03:41.700-03:00 level=INFO source=server.go:470 msg="system memory" total="31.9 GiB" free="11.4 GiB" free_swap="13.6 GiB"
time=2025-12-01T14:03:41.701-03:00 level=INFO source=server.go:522 msg=offload library=CUDA layers.requested=-1 layers.model=29 layers.offload=26 layers.split=[26] memory.available="[5.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.9 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.9 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
time=2025-12-01T14:03:41.743-03:00 level=INFO source=runner.go:910 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\zeNk0\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-haswell.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce GTX 1660 SUPER, compute capability 7.5, VMM: yes, ID: GPU-dadf7e43-b580-d107-63fb-993daaa7f187
The following devices will have suboptimal performance due to a lack of tensor cores:
  Device 0: NVIDIA GeForce GTX 1660 SUPER
Consider compiling with CMAKE_CUDA_ARCHITECTURES=61-virtual;80-virtual and DGGML_CUDA_FORCE_MMQ to force the use of the Pascal code for Turing.
load_backend: loaded CUDA backend from C:\Users\zeNk0\AppData\Local\Programs\Ollama\lib\ollama\cuda_v13\ggml-cuda.dll
time=2025-12-01T14:03:41.913-03:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-12-01T14:03:41.914-03:00 level=INFO source=runner.go:946 msg="Server listening on 127.0.0.1:64029"
time=2025-12-01T14:03:41.926-03:00 level=INFO source=runner.go:845 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:26[ID:GPU-dadf7e43-b580-d107-63fb-993daaa7f187 Layers:26(2..27)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-12-01T14:03:41.928-03:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-12-01T14:03:41.928-03:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce GTX 1660 SUPER) (0000:2b:00.0) - 5132 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from C:\Windows\system32\config\systemprofile\.ollama\models\blobs\sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-7...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7B
llama_model_loader: - kv  12:                               general.tags arr[str,2]       = ["chat", "text-generation"]
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 26 repeating layers to GPU
load_tensors: offloaded 26/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   710.78 MiB
load_tensors:        CUDA0 model buffer size =  3457.31 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache:      CUDA0 KV buffer size =   208.00 MiB
llama_kv_cache:        CPU KV buffer size =    16.00 MiB
llama_kv_cache: size =  224.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    17.01 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 32 (with bs=512), 3 (with bs=1)
time=2025-12-01T14:03:47.189-03:00 level=INFO source=server.go:1289 msg="llama runner started in 5.49 seconds"
time=2025-12-01T14:03:47.190-03:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-12-01T14:03:47.190-03:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-12-01T14:03:47.190-03:00 level=INFO source=server.go:1289 msg="llama runner started in 5.49 seconds"
time=2025-12-01T14:09:33.038-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59148"
time=2025-12-01T14:09:33.833-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59159"
time=2025-12-01T14:09:34.140-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59168"
time=2025-12-01T14:09:34.435-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59177"
time=2025-12-01T14:09:34.724-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59187"
time=2025-12-01T14:09:35.005-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59196"
time=2025-12-01T14:09:35.308-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59206"
time=2025-12-01T14:09:35.609-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59215"
time=2025-12-01T14:09:35.909-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59226"
time=2025-12-01T14:09:36.231-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59236"
time=2025-12-01T14:09:36.506-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59245"
time=2025-12-01T14:09:36.797-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59255"
time=2025-12-01T14:09:37.098-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59266"
time=2025-12-01T14:09:37.380-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59275"
time=2025-12-01T14:09:37.668-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59284"
time=2025-12-01T14:09:37.964-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59294"
time=2025-12-01T14:09:38.243-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59304"
time=2025-12-01T14:09:38.542-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59314"
time=2025-12-01T14:09:38.581-03:00 level=INFO source=runner.go:498 msg="failure during GPU discovery" OLLAMA_LIBRARY_PATH="[C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v13]" extra_envs=map[] error="failed to finish discovery before timeout"
time=2025-12-01T14:09:38.582-03:00 level=WARN source=runner.go:358 msg="unable to refresh free memory, using old values"
time=2025-12-01T14:33:46.770-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62706"
time=2025-12-01T14:33:47.176-03:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-12-01T14:33:47.176-03:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=8 efficiency=0 threads=16
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from C:\Windows\system32\config\systemprofile\.ollama\models\blobs\sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-7...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7B
llama_model_loader: - kv  12:                               general.tags arr[str,2]       = ["chat", "text-generation"]
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-12-01T14:33:47.542-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Windows\\system32\\config\\systemprofile\\.ollama\\models\\blobs\\sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 --port 62716"
time=2025-12-01T14:33:47.547-03:00 level=INFO source=server.go:470 msg="system memory" total="31.9 GiB" free="9.9 GiB" free_swap="12.9 GiB"
time=2025-12-01T14:33:47.548-03:00 level=INFO source=server.go:522 msg=offload library=CUDA layers.requested=-1 layers.model=29 layers.offload=26 layers.split=[26] memory.available="[5.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.9 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.9 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
time=2025-12-01T14:33:47.599-03:00 level=INFO source=runner.go:910 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\zeNk0\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-haswell.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce GTX 1660 SUPER, compute capability 7.5, VMM: yes, ID: GPU-dadf7e43-b580-d107-63fb-993daaa7f187
The following devices will have suboptimal performance due to a lack of tensor cores:
  Device 0: NVIDIA GeForce GTX 1660 SUPER
Consider compiling with CMAKE_CUDA_ARCHITECTURES=61-virtual;80-virtual and DGGML_CUDA_FORCE_MMQ to force the use of the Pascal code for Turing.
load_backend: loaded CUDA backend from C:\Users\zeNk0\AppData\Local\Programs\Ollama\lib\ollama\cuda_v13\ggml-cuda.dll
time=2025-12-01T14:33:47.785-03:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-12-01T14:33:47.786-03:00 level=INFO source=runner.go:946 msg="Server listening on 127.0.0.1:62716"
time=2025-12-01T14:33:47.794-03:00 level=INFO source=runner.go:845 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:26[ID:GPU-dadf7e43-b580-d107-63fb-993daaa7f187 Layers:26(2..27)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-12-01T14:33:47.794-03:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-12-01T14:33:47.794-03:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce GTX 1660 SUPER) (0000:2b:00.0) - 5132 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from C:\Windows\system32\config\systemprofile\.ollama\models\blobs\sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-7...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7B
llama_model_loader: - kv  12:                               general.tags arr[str,2]       = ["chat", "text-generation"]
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 26 repeating layers to GPU
load_tensors: offloaded 26/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   710.78 MiB
load_tensors:        CUDA0 model buffer size =  3457.31 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache:      CUDA0 KV buffer size =   208.00 MiB
llama_kv_cache:        CPU KV buffer size =    16.00 MiB
llama_kv_cache: size =  224.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    17.01 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 32 (with bs=512), 3 (with bs=1)
time=2025-12-01T14:33:57.564-03:00 level=INFO source=server.go:1289 msg="llama runner started in 10.02 seconds"
time=2025-12-01T14:33:57.565-03:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-12-01T14:33:57.565-03:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-12-01T14:33:57.566-03:00 level=INFO source=server.go:1289 msg="llama runner started in 10.02 seconds"
time=2025-12-01T14:39:14.676-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60386"
time=2025-12-01T14:39:15.386-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60396"
time=2025-12-01T14:39:15.670-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60406"
time=2025-12-01T14:39:15.953-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60416"
time=2025-12-01T14:39:16.221-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60425"
time=2025-12-01T14:39:16.479-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60435"
time=2025-12-01T14:39:16.747-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60444"
time=2025-12-01T14:39:17.013-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60453"
time=2025-12-01T14:39:17.285-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60462"
time=2025-12-01T14:39:17.550-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60470"
time=2025-12-01T14:39:17.810-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60479"
time=2025-12-01T14:39:18.058-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60488"
time=2025-12-01T14:39:18.318-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60497"
time=2025-12-01T14:39:18.593-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60506"
time=2025-12-01T14:39:18.882-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60515"
time=2025-12-01T14:39:19.152-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60524"
time=2025-12-01T14:39:19.421-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60533"
time=2025-12-01T14:39:19.690-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60542"
time=2025-12-01T14:39:19.956-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60551"
time=2025-12-01T14:39:20.135-03:00 level=INFO source=runner.go:498 msg="failure during GPU discovery" OLLAMA_LIBRARY_PATH="[C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v13]" extra_envs=map[] error="failed to finish discovery before timeout"
time=2025-12-01T14:39:20.135-03:00 level=WARN source=runner.go:358 msg="unable to refresh free memory, using old values"
time=2025-12-01T14:39:20.137-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60560"
time=2025-12-01T14:39:20.140-03:00 level=INFO source=runner.go:498 msg="failure during GPU discovery" OLLAMA_LIBRARY_PATH="[C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v13]" extra_envs=map[] error="failed to finish discovery before timeout"
time=2025-12-01T14:39:20.140-03:00 level=WARN source=runner.go:358 msg="unable to refresh free memory, using old values"
time=2025-12-01T14:45:21.426-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63779"
time=2025-12-01T14:45:21.751-03:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-12-01T14:45:21.751-03:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=8 efficiency=0 threads=16
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from C:\Windows\system32\config\systemprofile\.ollama\models\blobs\sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-7...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7B
llama_model_loader: - kv  12:                               general.tags arr[str,2]       = ["chat", "text-generation"]
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-12-01T14:45:22.051-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Windows\\system32\\config\\systemprofile\\.ollama\\models\\blobs\\sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 --port 63790"
time=2025-12-01T14:45:22.056-03:00 level=INFO source=server.go:470 msg="system memory" total="31.9 GiB" free="9.4 GiB" free_swap="12.7 GiB"
time=2025-12-01T14:45:22.056-03:00 level=INFO source=server.go:522 msg=offload library=CUDA layers.requested=-1 layers.model=29 layers.offload=26 layers.split=[26] memory.available="[5.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.9 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.9 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
time=2025-12-01T14:45:22.102-03:00 level=INFO source=runner.go:910 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\zeNk0\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-haswell.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce GTX 1660 SUPER, compute capability 7.5, VMM: yes, ID: GPU-dadf7e43-b580-d107-63fb-993daaa7f187
The following devices will have suboptimal performance due to a lack of tensor cores:
  Device 0: NVIDIA GeForce GTX 1660 SUPER
Consider compiling with CMAKE_CUDA_ARCHITECTURES=61-virtual;80-virtual and DGGML_CUDA_FORCE_MMQ to force the use of the Pascal code for Turing.
load_backend: loaded CUDA backend from C:\Users\zeNk0\AppData\Local\Programs\Ollama\lib\ollama\cuda_v13\ggml-cuda.dll
time=2025-12-01T14:45:22.227-03:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-12-01T14:45:22.228-03:00 level=INFO source=runner.go:946 msg="Server listening on 127.0.0.1:63790"
time=2025-12-01T14:45:22.233-03:00 level=INFO source=runner.go:845 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:26[ID:GPU-dadf7e43-b580-d107-63fb-993daaa7f187 Layers:26(2..27)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-12-01T14:45:22.234-03:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-12-01T14:45:22.234-03:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce GTX 1660 SUPER) (0000:2b:00.0) - 5132 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from C:\Windows\system32\config\systemprofile\.ollama\models\blobs\sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-7...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7B
llama_model_loader: - kv  12:                               general.tags arr[str,2]       = ["chat", "text-generation"]
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 26 repeating layers to GPU
load_tensors: offloaded 26/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   710.78 MiB
load_tensors:        CUDA0 model buffer size =  3457.31 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache:      CUDA0 KV buffer size =   208.00 MiB
llama_kv_cache:        CPU KV buffer size =    16.00 MiB
llama_kv_cache: size =  224.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    17.01 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 32 (with bs=512), 3 (with bs=1)
time=2025-12-01T14:45:24.488-03:00 level=INFO source=server.go:1289 msg="llama runner started in 2.43 seconds"
time=2025-12-01T14:45:24.488-03:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-12-01T14:45:24.488-03:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-12-01T14:45:24.488-03:00 level=INFO source=server.go:1289 msg="llama runner started in 2.43 seconds"
time=2025-12-01T14:50:41.024-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63705"
time=2025-12-01T14:50:42.610-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63714"
time=2025-12-01T14:50:42.905-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63722"
time=2025-12-01T14:50:43.150-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63730"
time=2025-12-01T14:50:43.402-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63739"
time=2025-12-01T14:50:43.652-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63748"
time=2025-12-01T14:50:43.893-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63756"
time=2025-12-01T14:50:44.135-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63765"
time=2025-12-01T14:50:44.379-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63773"
time=2025-12-01T14:50:44.632-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63782"
time=2025-12-01T14:50:44.868-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63790"
time=2025-12-01T14:50:45.118-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63799"
time=2025-12-01T14:50:45.360-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63807"
time=2025-12-01T14:50:45.610-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63816"
time=2025-12-01T14:50:45.860-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63824"
time=2025-12-01T14:50:46.114-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63832"
time=2025-12-01T14:50:46.360-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63841"
time=2025-12-01T14:50:46.612-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63850"
time=2025-12-01T14:50:46.861-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63859"
time=2025-12-01T14:50:47.110-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63867"
time=2025-12-01T14:50:47.359-03:00 level=INFO source=runner.go:498 msg="failure during GPU discovery" OLLAMA_LIBRARY_PATH="[C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v13]" extra_envs=map[] error="failed to finish discovery before timeout"
time=2025-12-01T14:50:47.359-03:00 level=WARN source=runner.go:358 msg="unable to refresh free memory, using old values"
time=2025-12-01T14:53:01.631-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 53843"
time=2025-12-01T14:53:01.853-03:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-12-01T14:53:01.853-03:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=8 efficiency=0 threads=16
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from C:\Windows\system32\config\systemprofile\.ollama\models\blobs\sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-7...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7B
llama_model_loader: - kv  12:                               general.tags arr[str,2]       = ["chat", "text-generation"]
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-12-01T14:53:02.110-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Windows\\system32\\config\\systemprofile\\.ollama\\models\\blobs\\sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 --port 53850"
time=2025-12-01T14:53:02.112-03:00 level=INFO source=server.go:470 msg="system memory" total="31.9 GiB" free="9.0 GiB" free_swap="13.0 GiB"
time=2025-12-01T14:53:02.112-03:00 level=INFO source=server.go:522 msg=offload library=CUDA layers.requested=-1 layers.model=29 layers.offload=26 layers.split=[26] memory.available="[5.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.9 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.9 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
time=2025-12-01T14:53:02.147-03:00 level=INFO source=runner.go:910 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\zeNk0\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-haswell.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce GTX 1660 SUPER, compute capability 7.5, VMM: yes, ID: GPU-dadf7e43-b580-d107-63fb-993daaa7f187
The following devices will have suboptimal performance due to a lack of tensor cores:
  Device 0: NVIDIA GeForce GTX 1660 SUPER
Consider compiling with CMAKE_CUDA_ARCHITECTURES=61-virtual;80-virtual and DGGML_CUDA_FORCE_MMQ to force the use of the Pascal code for Turing.
load_backend: loaded CUDA backend from C:\Users\zeNk0\AppData\Local\Programs\Ollama\lib\ollama\cuda_v13\ggml-cuda.dll
time=2025-12-01T14:53:02.270-03:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-12-01T14:53:02.270-03:00 level=INFO source=runner.go:946 msg="Server listening on 127.0.0.1:53850"
time=2025-12-01T14:53:02.278-03:00 level=INFO source=runner.go:845 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:26[ID:GPU-dadf7e43-b580-d107-63fb-993daaa7f187 Layers:26(2..27)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-12-01T14:53:02.279-03:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-12-01T14:53:02.279-03:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce GTX 1660 SUPER) (0000:2b:00.0) - 5132 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from C:\Windows\system32\config\systemprofile\.ollama\models\blobs\sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-7...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7B
llama_model_loader: - kv  12:                               general.tags arr[str,2]       = ["chat", "text-generation"]
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 26 repeating layers to GPU
load_tensors: offloaded 26/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   710.78 MiB
load_tensors:        CUDA0 model buffer size =  3457.31 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache:      CUDA0 KV buffer size =   208.00 MiB
llama_kv_cache:        CPU KV buffer size =    16.00 MiB
llama_kv_cache: size =  224.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    17.01 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 32 (with bs=512), 3 (with bs=1)
time=2025-12-01T14:53:06.537-03:00 level=INFO source=server.go:1289 msg="llama runner started in 4.43 seconds"
time=2025-12-01T14:53:06.537-03:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-12-01T14:53:06.537-03:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-12-01T14:53:06.537-03:00 level=INFO source=server.go:1289 msg="llama runner started in 4.43 seconds"
time=2025-12-01T14:58:15.947-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 61711"
time=2025-12-01T14:58:16.656-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 61720"
time=2025-12-01T14:58:16.911-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 61728"
time=2025-12-01T14:58:17.168-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 61736"
time=2025-12-01T14:58:17.449-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 61746"
time=2025-12-01T14:58:17.729-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 61755"
time=2025-12-01T14:58:18.010-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 61764"
time=2025-12-01T14:58:18.289-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 61773"
time=2025-12-01T14:58:18.563-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 61782"
time=2025-12-01T14:58:18.826-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 61791"
time=2025-12-01T14:58:19.090-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 61800"
time=2025-12-01T14:58:19.365-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 61809"
time=2025-12-01T14:58:19.645-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 61818"
time=2025-12-01T14:58:19.914-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 61826"
time=2025-12-01T14:58:20.168-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 61834"
time=2025-12-01T14:58:20.446-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 61842"
time=2025-12-01T14:58:20.719-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 61851"
time=2025-12-01T14:58:20.987-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 61860"
time=2025-12-01T14:58:21.269-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 61870"
time=2025-12-01T14:58:21.405-03:00 level=INFO source=runner.go:498 msg="failure during GPU discovery" OLLAMA_LIBRARY_PATH="[C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v13]" extra_envs=map[] error="failed to finish discovery before timeout"
time=2025-12-01T14:58:21.405-03:00 level=WARN source=runner.go:358 msg="unable to refresh free memory, using old values"
time=2025-12-01T15:49:54.522-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 57338"
time=2025-12-01T15:49:55.385-03:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-12-01T15:49:55.385-03:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=8 efficiency=0 threads=16
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from C:\Windows\system32\config\systemprofile\.ollama\models\blobs\sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-7...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7B
llama_model_loader: - kv  12:                               general.tags arr[str,2]       = ["chat", "text-generation"]
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-12-01T15:49:55.702-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Windows\\system32\\config\\systemprofile\\.ollama\\models\\blobs\\sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 --port 57347"
time=2025-12-01T15:49:55.705-03:00 level=INFO source=server.go:470 msg="system memory" total="31.9 GiB" free="7.8 GiB" free_swap="11.4 GiB"
time=2025-12-01T15:49:55.705-03:00 level=INFO source=server.go:522 msg=offload library=CUDA layers.requested=-1 layers.model=29 layers.offload=26 layers.split=[26] memory.available="[5.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.9 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.9 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
time=2025-12-01T15:49:55.746-03:00 level=INFO source=runner.go:910 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\zeNk0\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-haswell.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce GTX 1660 SUPER, compute capability 7.5, VMM: yes, ID: GPU-dadf7e43-b580-d107-63fb-993daaa7f187
The following devices will have suboptimal performance due to a lack of tensor cores:
  Device 0: NVIDIA GeForce GTX 1660 SUPER
Consider compiling with CMAKE_CUDA_ARCHITECTURES=61-virtual;80-virtual and DGGML_CUDA_FORCE_MMQ to force the use of the Pascal code for Turing.
load_backend: loaded CUDA backend from C:\Users\zeNk0\AppData\Local\Programs\Ollama\lib\ollama\cuda_v13\ggml-cuda.dll
time=2025-12-01T15:49:55.879-03:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-12-01T15:49:55.880-03:00 level=INFO source=runner.go:946 msg="Server listening on 127.0.0.1:57347"
time=2025-12-01T15:49:55.886-03:00 level=INFO source=runner.go:845 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:26[ID:GPU-dadf7e43-b580-d107-63fb-993daaa7f187 Layers:26(2..27)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-12-01T15:49:55.887-03:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-12-01T15:49:55.887-03:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce GTX 1660 SUPER) (0000:2b:00.0) - 5132 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from C:\Windows\system32\config\systemprofile\.ollama\models\blobs\sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-7...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7B
llama_model_loader: - kv  12:                               general.tags arr[str,2]       = ["chat", "text-generation"]
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 26 repeating layers to GPU
load_tensors: offloaded 26/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   710.78 MiB
load_tensors:        CUDA0 model buffer size =  3457.31 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache:      CUDA0 KV buffer size =   208.00 MiB
llama_kv_cache:        CPU KV buffer size =    16.00 MiB
llama_kv_cache: size =  224.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    17.01 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 32 (with bs=512), 3 (with bs=1)
time=2025-12-01T15:50:00.897-03:00 level=INFO source=server.go:1289 msg="llama runner started in 5.19 seconds"
time=2025-12-01T15:50:00.897-03:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-12-01T15:50:00.898-03:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-12-01T15:50:00.898-03:00 level=INFO source=server.go:1289 msg="llama runner started in 5.19 seconds"
time=2025-12-01T15:55:19.560-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62597"
time=2025-12-01T15:55:20.271-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62606"
time=2025-12-01T15:55:20.579-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62615"
time=2025-12-01T15:55:20.861-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62624"
time=2025-12-01T15:55:21.127-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62633"
time=2025-12-01T15:55:21.409-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62642"
time=2025-12-01T15:55:21.679-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62651"
time=2025-12-01T15:55:21.942-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62660"
time=2025-12-01T15:55:22.232-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62669"
time=2025-12-01T15:55:22.506-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62678"
time=2025-12-01T15:55:22.774-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62687"
time=2025-12-01T15:55:23.039-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62697"
time=2025-12-01T15:55:23.314-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62706"
time=2025-12-01T15:55:23.573-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62716"
time=2025-12-01T15:55:23.875-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62727"
time=2025-12-01T15:55:24.136-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62735"
time=2025-12-01T15:55:24.398-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62744"
time=2025-12-01T15:55:24.669-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62753"
time=2025-12-01T15:55:24.937-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62761"
time=2025-12-01T15:55:25.019-03:00 level=INFO source=runner.go:498 msg="failure during GPU discovery" OLLAMA_LIBRARY_PATH="[C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v13]" extra_envs=map[] error="failed to finish discovery before timeout"
time=2025-12-01T15:55:25.019-03:00 level=WARN source=runner.go:358 msg="unable to refresh free memory, using old values"
time=2025-12-01T19:10:58.717-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 63902"
time=2025-12-01T19:10:59.558-03:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-12-01T19:10:59.558-03:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=8 efficiency=0 threads=16
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from C:\Windows\system32\config\systemprofile\.ollama\models\blobs\sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-7...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7B
llama_model_loader: - kv  12:                               general.tags arr[str,2]       = ["chat", "text-generation"]
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-12-01T19:10:59.866-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Windows\\system32\\config\\systemprofile\\.ollama\\models\\blobs\\sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 --port 63910"
time=2025-12-01T19:10:59.868-03:00 level=INFO source=server.go:470 msg="system memory" total="31.9 GiB" free="3.4 GiB" free_swap="5.4 GiB"
time=2025-12-01T19:10:59.869-03:00 level=INFO source=server.go:522 msg=offload library=CUDA layers.requested=-1 layers.model=29 layers.offload=26 layers.split=[26] memory.available="[5.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.9 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.9 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
time=2025-12-01T19:10:59.912-03:00 level=INFO source=runner.go:910 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\zeNk0\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-haswell.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce GTX 1660 SUPER, compute capability 7.5, VMM: yes, ID: GPU-dadf7e43-b580-d107-63fb-993daaa7f187
The following devices will have suboptimal performance due to a lack of tensor cores:
  Device 0: NVIDIA GeForce GTX 1660 SUPER
Consider compiling with CMAKE_CUDA_ARCHITECTURES=61-virtual;80-virtual and DGGML_CUDA_FORCE_MMQ to force the use of the Pascal code for Turing.
load_backend: loaded CUDA backend from C:\Users\zeNk0\AppData\Local\Programs\Ollama\lib\ollama\cuda_v13\ggml-cuda.dll
time=2025-12-01T19:11:00.204-03:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-12-01T19:11:00.205-03:00 level=INFO source=runner.go:946 msg="Server listening on 127.0.0.1:63910"
time=2025-12-01T19:11:00.208-03:00 level=INFO source=runner.go:845 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:26[ID:GPU-dadf7e43-b580-d107-63fb-993daaa7f187 Layers:26(2..27)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-12-01T19:11:00.209-03:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-12-01T19:11:00.209-03:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce GTX 1660 SUPER) (0000:2b:00.0) - 5132 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from C:\Windows\system32\config\systemprofile\.ollama\models\blobs\sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-7...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7B
llama_model_loader: - kv  12:                               general.tags arr[str,2]       = ["chat", "text-generation"]
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 26 repeating layers to GPU
load_tensors: offloaded 26/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   710.78 MiB
load_tensors:        CUDA0 model buffer size =  3457.31 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache:      CUDA0 KV buffer size =   208.00 MiB
llama_kv_cache:        CPU KV buffer size =    16.00 MiB
llama_kv_cache: size =  224.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    17.01 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 32 (with bs=512), 3 (with bs=1)
time=2025-12-01T19:11:05.721-03:00 level=INFO source=server.go:1289 msg="llama runner started in 5.85 seconds"
time=2025-12-01T19:11:05.721-03:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-12-01T19:11:05.722-03:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-12-01T19:11:05.722-03:00 level=INFO source=server.go:1289 msg="llama runner started in 5.85 seconds"
time=2025-12-01T19:17:06.557-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60298"
time=2025-12-01T19:17:07.195-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60307"
time=2025-12-01T19:17:07.445-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62698"
time=2025-12-01T19:17:07.694-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62707"
time=2025-12-01T19:17:07.945-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62715"
time=2025-12-01T19:17:08.195-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62723"
time=2025-12-01T19:17:08.445-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62731"
time=2025-12-01T19:17:08.695-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62738"
time=2025-12-01T19:17:08.945-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62745"
time=2025-12-01T19:17:09.194-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62752"
time=2025-12-01T19:17:09.445-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62760"
time=2025-12-01T19:17:09.694-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62769"
time=2025-12-01T19:17:09.945-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62779"
time=2025-12-01T19:17:10.194-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62788"
time=2025-12-01T19:17:10.468-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 52141"
time=2025-12-01T19:17:10.718-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 52150"
time=2025-12-01T19:17:10.970-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 52159"
time=2025-12-01T19:17:11.227-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 52168"
time=2025-12-01T19:17:11.494-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 52178"
time=2025-12-01T19:17:11.759-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 52187"
time=2025-12-01T19:17:11.943-03:00 level=INFO source=runner.go:498 msg="failure during GPU discovery" OLLAMA_LIBRARY_PATH="[C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v13]" extra_envs=map[] error="failed to finish discovery before timeout"
time=2025-12-01T19:17:11.943-03:00 level=WARN source=runner.go:358 msg="unable to refresh free memory, using old values"
time=2025-12-01T19:17:11.945-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 52196"
time=2025-12-01T19:17:11.947-03:00 level=INFO source=runner.go:498 msg="failure during GPU discovery" OLLAMA_LIBRARY_PATH="[C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v13]" extra_envs=map[] error="failed to finish discovery before timeout"
time=2025-12-01T19:17:11.947-03:00 level=WARN source=runner.go:358 msg="unable to refresh free memory, using old values"
time=2025-12-02T01:53:59.316-03:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Windows\\system32\\config\\systemprofile\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-12-02T01:53:59.345-03:00 level=INFO source=images.go:522 msg="total blobs: 10"
time=2025-12-02T01:53:59.346-03:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-12-02T01:53:59.347-03:00 level=INFO source=routes.go:1577 msg="Listening on 127.0.0.1:11434 (version 0.12.9)"
time=2025-12-02T01:53:59.349-03:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-12-02T01:53:59.357-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 55976"
time=2025-12-02T01:53:59.552-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 55985"
time=2025-12-02T01:53:59.905-03:00 level=INFO source=runner.go:498 msg="failure during GPU discovery" OLLAMA_LIBRARY_PATH="[C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v12]" extra_envs=map[] error="failed to finish discovery before timeout"
time=2025-12-02T01:53:59.907-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 55991"
time=2025-12-02T01:53:59.909-03:00 level=INFO source=runner.go:498 msg="failure during GPU discovery" OLLAMA_LIBRARY_PATH="[C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v13]" extra_envs=map[] error="failed to finish discovery before timeout"
time=2025-12-02T01:53:59.909-03:00 level=INFO source=types.go:60 msg="inference compute" id=cpu library=cpu compute="" name=cpu description=cpu libdirs=ollama driver="" pci_id="" type="" total="31.9 GiB" available="24.5 GiB"
time=2025-12-02T01:53:59.909-03:00 level=INFO source=routes.go:1618 msg="entering low vram mode" "total vram"="0 B" threshold="20.0 GiB"
time=2025-12-02T15:23:23.936-03:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Windows\\system32\\config\\systemprofile\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-12-02T15:23:24.056-03:00 level=INFO source=images.go:522 msg="total blobs: 10"
time=2025-12-02T15:23:24.059-03:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-12-02T15:23:24.064-03:00 level=INFO source=routes.go:1577 msg="Listening on 127.0.0.1:11434 (version 0.12.9)"
time=2025-12-02T15:23:24.071-03:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-12-02T15:23:24.085-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 49696"
time=2025-12-02T15:23:26.259-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 64465"
time=2025-12-02T15:23:27.072-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 52520"
time=2025-12-02T15:23:27.342-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 52534"
time=2025-12-02T15:23:27.342-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 52535"
time=2025-12-02T15:23:27.708-03:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-dadf7e43-b580-d107-63fb-993daaa7f187 filtered_id="" library=CUDA compute=7.5 name=CUDA0 description="NVIDIA GeForce GTX 1660 SUPER" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:2b:00.0 type=discrete total="6.0 GiB" available="5.0 GiB"
time=2025-12-02T15:23:27.709-03:00 level=INFO source=routes.go:1618 msg="entering low vram mode" "total vram"="6.0 GiB" threshold="20.0 GiB"
time=2025-12-02T18:58:24.047-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 58657"
time=2025-12-02T18:58:24.766-03:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-12-02T18:58:24.767-03:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=8 efficiency=0 threads=16
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from C:\Windows\system32\config\systemprofile\.ollama\models\blobs\sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-7...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7B
llama_model_loader: - kv  12:                               general.tags arr[str,2]       = ["chat", "text-generation"]
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-12-02T18:58:25.055-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Windows\\system32\\config\\systemprofile\\.ollama\\models\\blobs\\sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 --port 56889"
time=2025-12-02T18:58:25.057-03:00 level=INFO source=server.go:470 msg="system memory" total="31.9 GiB" free="5.6 GiB" free_swap="7.3 GiB"
time=2025-12-02T18:58:25.058-03:00 level=INFO source=server.go:522 msg=offload library=CUDA layers.requested=-1 layers.model=29 layers.offload=26 layers.split=[26] memory.available="[5.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.9 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.9 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
time=2025-12-02T18:58:25.097-03:00 level=INFO source=runner.go:910 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\zeNk0\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-haswell.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce GTX 1660 SUPER, compute capability 7.5, VMM: yes, ID: GPU-dadf7e43-b580-d107-63fb-993daaa7f187
The following devices will have suboptimal performance due to a lack of tensor cores:
  Device 0: NVIDIA GeForce GTX 1660 SUPER
Consider compiling with CMAKE_CUDA_ARCHITECTURES=61-virtual;80-virtual and DGGML_CUDA_FORCE_MMQ to force the use of the Pascal code for Turing.
load_backend: loaded CUDA backend from C:\Users\zeNk0\AppData\Local\Programs\Ollama\lib\ollama\cuda_v13\ggml-cuda.dll
time=2025-12-02T18:58:25.211-03:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-12-02T18:58:25.212-03:00 level=INFO source=runner.go:946 msg="Server listening on 127.0.0.1:56889"
time=2025-12-02T18:58:25.213-03:00 level=INFO source=runner.go:845 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:26[ID:GPU-dadf7e43-b580-d107-63fb-993daaa7f187 Layers:26(2..27)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-12-02T18:58:25.215-03:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-12-02T18:58:25.215-03:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce GTX 1660 SUPER) (0000:2b:00.0) - 5132 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from C:\Windows\system32\config\systemprofile\.ollama\models\blobs\sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-7...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7B
llama_model_loader: - kv  12:                               general.tags arr[str,2]       = ["chat", "text-generation"]
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 26 repeating layers to GPU
load_tensors: offloaded 26/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   710.78 MiB
load_tensors:        CUDA0 model buffer size =  3457.31 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache:      CUDA0 KV buffer size =   208.00 MiB
llama_kv_cache:        CPU KV buffer size =    16.00 MiB
llama_kv_cache: size =  224.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    17.01 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 32 (with bs=512), 3 (with bs=1)
time=2025-12-02T18:58:32.979-03:00 level=INFO source=server.go:1289 msg="llama runner started in 7.92 seconds"
time=2025-12-02T18:58:32.980-03:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-12-02T18:58:32.980-03:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-12-02T18:58:32.981-03:00 level=INFO source=server.go:1289 msg="llama runner started in 7.92 seconds"
time=2025-12-02T19:03:46.089-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 58976"
time=2025-12-02T19:03:46.880-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 58985"
time=2025-12-02T19:03:47.153-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 58994"
time=2025-12-02T19:03:47.453-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59004"
time=2025-12-02T19:03:47.737-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59014"
time=2025-12-02T19:03:47.999-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59023"
time=2025-12-02T19:03:48.255-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59032"
time=2025-12-02T19:03:48.547-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59042"
time=2025-12-02T19:03:48.817-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59051"
time=2025-12-02T19:03:49.084-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59061"
time=2025-12-02T19:03:49.370-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59071"
time=2025-12-02T19:03:49.634-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59080"
time=2025-12-02T19:03:49.902-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59089"
time=2025-12-02T19:03:50.158-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59098"
time=2025-12-02T19:03:50.424-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59107"
time=2025-12-02T19:03:50.687-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59116"
time=2025-12-02T19:03:50.953-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59126"
time=2025-12-02T19:03:51.236-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59136"
time=2025-12-02T19:03:51.508-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 59145"
time=2025-12-02T19:03:51.629-03:00 level=INFO source=runner.go:498 msg="failure during GPU discovery" OLLAMA_LIBRARY_PATH="[C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v13]" extra_envs=map[] error="failed to finish discovery before timeout"
time=2025-12-02T19:03:51.629-03:00 level=WARN source=runner.go:358 msg="unable to refresh free memory, using old values"
time=2025-12-02T23:41:37.093-03:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Windows\\system32\\config\\systemprofile\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-12-02T23:41:37.138-03:00 level=INFO source=images.go:522 msg="total blobs: 10"
time=2025-12-02T23:41:37.139-03:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-12-02T23:41:37.140-03:00 level=INFO source=routes.go:1577 msg="Listening on 127.0.0.1:11434 (version 0.12.9)"
time=2025-12-02T23:41:37.142-03:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-12-02T23:41:37.150-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 50351"
time=2025-12-02T23:41:37.367-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 50358"
time=2025-12-02T23:41:38.001-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 50366"
time=2025-12-02T23:41:38.501-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 50373"
time=2025-12-02T23:41:38.501-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 50374"
time=2025-12-02T23:41:38.735-03:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-dadf7e43-b580-d107-63fb-993daaa7f187 filtered_id="" library=CUDA compute=7.5 name=CUDA0 description="NVIDIA GeForce GTX 1660 SUPER" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:2b:00.0 type=discrete total="6.0 GiB" available="5.0 GiB"
time=2025-12-02T23:41:38.735-03:00 level=INFO source=routes.go:1618 msg="entering low vram mode" "total vram"="6.0 GiB" threshold="20.0 GiB"
time=2025-12-02T23:42:17.260-03:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Windows\\system32\\config\\systemprofile\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-12-02T23:42:17.358-03:00 level=INFO source=images.go:522 msg="total blobs: 10"
time=2025-12-02T23:42:17.359-03:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-12-02T23:42:17.362-03:00 level=INFO source=routes.go:1577 msg="Listening on 127.0.0.1:11434 (version 0.12.9)"
time=2025-12-02T23:42:17.365-03:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-12-02T23:42:17.378-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 49674"
time=2025-12-02T23:42:17.694-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 49686"
time=2025-12-02T23:42:18.699-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 49704"
time=2025-12-02T23:42:19.550-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 49719"
time=2025-12-02T23:42:19.552-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 49720"
time=2025-12-02T23:42:19.930-03:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-dadf7e43-b580-d107-63fb-993daaa7f187 filtered_id="" library=CUDA compute=7.5 name=CUDA0 description="NVIDIA GeForce GTX 1660 SUPER" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:2b:00.0 type=discrete total="6.0 GiB" available="5.0 GiB"
time=2025-12-02T23:42:19.931-03:00 level=INFO source=routes.go:1618 msg="entering low vram mode" "total vram"="6.0 GiB" threshold="20.0 GiB"
time=2025-12-03T21:42:29.461-03:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Windows\\system32\\config\\systemprofile\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-12-03T21:42:29.546-03:00 level=INFO source=images.go:522 msg="total blobs: 10"
time=2025-12-03T21:42:29.549-03:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-12-03T21:42:29.552-03:00 level=INFO source=routes.go:1577 msg="Listening on 127.0.0.1:11434 (version 0.12.9)"
time=2025-12-03T21:42:29.559-03:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-12-03T21:42:29.588-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 49689"
time=2025-12-03T21:42:30.783-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 51703"
time=2025-12-03T21:42:31.541-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 57637"
time=2025-12-03T21:42:31.840-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 54657"
time=2025-12-03T21:42:31.844-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 54658"
time=2025-12-03T21:42:32.271-03:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-dadf7e43-b580-d107-63fb-993daaa7f187 filtered_id="" library=CUDA compute=7.5 name=CUDA0 description="NVIDIA GeForce GTX 1660 SUPER" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:2b:00.0 type=discrete total="6.0 GiB" available="5.0 GiB"
time=2025-12-03T21:42:32.271-03:00 level=INFO source=routes.go:1618 msg="entering low vram mode" "total vram"="6.0 GiB" threshold="20.0 GiB"
time=2025-12-03T22:04:46.726-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60577"
time=2025-12-03T22:04:47.048-03:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-12-03T22:04:47.049-03:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=8 efficiency=0 threads=16
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from C:\Windows\system32\config\systemprofile\.ollama\models\blobs\sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-7...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7B
llama_model_loader: - kv  12:                               general.tags arr[str,2]       = ["chat", "text-generation"]
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-12-03T22:04:47.230-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Windows\\system32\\config\\systemprofile\\.ollama\\models\\blobs\\sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 --port 60591"
time=2025-12-03T22:04:47.234-03:00 level=INFO source=server.go:470 msg="system memory" total="31.9 GiB" free="22.0 GiB" free_swap="28.3 GiB"
time=2025-12-03T22:04:47.235-03:00 level=INFO source=server.go:522 msg=offload library=CUDA layers.requested=-1 layers.model=29 layers.offload=26 layers.split=[26] memory.available="[5.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.9 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.9 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
time=2025-12-03T22:04:47.264-03:00 level=INFO source=runner.go:910 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\zeNk0\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-haswell.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce GTX 1660 SUPER, compute capability 7.5, VMM: yes, ID: GPU-dadf7e43-b580-d107-63fb-993daaa7f187
The following devices will have suboptimal performance due to a lack of tensor cores:
  Device 0: NVIDIA GeForce GTX 1660 SUPER
Consider compiling with CMAKE_CUDA_ARCHITECTURES=61-virtual;80-virtual and DGGML_CUDA_FORCE_MMQ to force the use of the Pascal code for Turing.
load_backend: loaded CUDA backend from C:\Users\zeNk0\AppData\Local\Programs\Ollama\lib\ollama\cuda_v13\ggml-cuda.dll
time=2025-12-03T22:04:47.374-03:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-12-03T22:04:47.375-03:00 level=INFO source=runner.go:946 msg="Server listening on 127.0.0.1:60591"
time=2025-12-03T22:04:47.385-03:00 level=INFO source=runner.go:845 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:26[ID:GPU-dadf7e43-b580-d107-63fb-993daaa7f187 Layers:26(2..27)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-12-03T22:04:47.386-03:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-12-03T22:04:47.386-03:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce GTX 1660 SUPER) (0000:2b:00.0) - 5132 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from C:\Windows\system32\config\systemprofile\.ollama\models\blobs\sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-7...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7B
llama_model_loader: - kv  12:                               general.tags arr[str,2]       = ["chat", "text-generation"]
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 26 repeating layers to GPU
load_tensors: offloaded 26/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   710.78 MiB
load_tensors:        CUDA0 model buffer size =  3457.31 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache:      CUDA0 KV buffer size =   208.00 MiB
llama_kv_cache:        CPU KV buffer size =    16.00 MiB
llama_kv_cache: size =  224.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    17.01 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 32 (with bs=512), 3 (with bs=1)
time=2025-12-03T22:04:50.390-03:00 level=INFO source=server.go:1289 msg="llama runner started in 3.16 seconds"
time=2025-12-03T22:04:50.390-03:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-12-03T22:04:50.390-03:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-12-03T22:04:50.391-03:00 level=INFO source=server.go:1289 msg="llama runner started in 3.16 seconds"
time=2025-12-03T22:24:12.608-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60718"
time=2025-12-03T22:24:13.087-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60726"
time=2025-12-03T22:24:13.337-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60733"
time=2025-12-03T22:24:13.587-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60740"
time=2025-12-03T22:24:13.837-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60747"
time=2025-12-03T22:24:14.086-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60754"
time=2025-12-03T22:24:14.337-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60761"
time=2025-12-03T22:24:14.587-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60768"
time=2025-12-03T22:24:14.838-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60775"
time=2025-12-03T22:24:15.087-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60782"
time=2025-12-03T22:24:15.336-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60789"
time=2025-12-03T22:24:15.587-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60796"
time=2025-12-03T22:24:15.837-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60803"
time=2025-12-03T22:24:16.088-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60810"
time=2025-12-03T22:24:16.338-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60822"
time=2025-12-03T22:24:16.588-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60835"
time=2025-12-03T22:24:16.837-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60842"
time=2025-12-03T22:24:17.087-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60849"
time=2025-12-03T22:24:17.337-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60856"
time=2025-12-03T22:24:17.587-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60864"
time=2025-12-03T22:24:17.841-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 60871"
time=2025-12-03T22:24:17.843-03:00 level=INFO source=runner.go:498 msg="failure during GPU discovery" OLLAMA_LIBRARY_PATH="[C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v13]" extra_envs=map[] error="failed to finish discovery before timeout"
time=2025-12-03T22:24:17.843-03:00 level=WARN source=runner.go:358 msg="unable to refresh free memory, using old values"
time=2025-12-03T23:28:03.893-03:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Windows\\system32\\config\\systemprofile\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-12-03T23:28:03.921-03:00 level=INFO source=images.go:522 msg="total blobs: 10"
time=2025-12-03T23:28:03.921-03:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-12-03T23:28:03.922-03:00 level=INFO source=routes.go:1577 msg="Listening on 127.0.0.1:11434 (version 0.12.9)"
time=2025-12-03T23:28:03.923-03:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-12-03T23:28:03.932-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62696"
time=2025-12-03T23:28:04.799-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62703"
time=2025-12-03T23:28:05.453-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62712"
time=2025-12-03T23:28:05.674-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62721"
time=2025-12-03T23:28:05.674-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 62720"
time=2025-12-03T23:28:06.105-03:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-dadf7e43-b580-d107-63fb-993daaa7f187 filtered_id="" library=CUDA compute=7.5 name=CUDA0 description="NVIDIA GeForce GTX 1660 SUPER" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:2b:00.0 type=discrete total="6.0 GiB" available="5.0 GiB"
time=2025-12-03T23:28:06.105-03:00 level=INFO source=routes.go:1618 msg="entering low vram mode" "total vram"="6.0 GiB" threshold="20.0 GiB"
time=2025-12-03T23:34:43.608-03:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Windows\\system32\\config\\systemprofile\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-12-03T23:34:43.633-03:00 level=INFO source=images.go:522 msg="total blobs: 10"
time=2025-12-03T23:34:43.634-03:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-12-03T23:34:43.634-03:00 level=INFO source=routes.go:1577 msg="Listening on 127.0.0.1:11434 (version 0.12.9)"
time=2025-12-03T23:34:43.635-03:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-12-03T23:34:43.645-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 54460"
time=2025-12-03T23:34:44.010-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 54467"
time=2025-12-03T23:34:44.281-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 54475"
time=2025-12-03T23:34:44.395-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 54485"
time=2025-12-03T23:34:44.395-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 54486"
time=2025-12-03T23:34:44.995-03:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-dadf7e43-b580-d107-63fb-993daaa7f187 filtered_id="" library=CUDA compute=7.5 name=CUDA0 description="NVIDIA GeForce GTX 1660 SUPER" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:2b:00.0 type=discrete total="6.0 GiB" available="5.0 GiB"
time=2025-12-03T23:34:44.995-03:00 level=INFO source=routes.go:1618 msg="entering low vram mode" "total vram"="6.0 GiB" threshold="20.0 GiB"
time=2025-12-04T23:14:33.936-03:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Windows\\system32\\config\\systemprofile\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-12-04T23:14:34.087-03:00 level=INFO source=images.go:522 msg="total blobs: 10"
time=2025-12-04T23:14:34.089-03:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-12-04T23:14:34.099-03:00 level=INFO source=routes.go:1577 msg="Listening on 127.0.0.1:11434 (version 0.12.9)"
time=2025-12-04T23:14:34.109-03:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-12-04T23:14:34.131-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 57500"
time=2025-12-04T23:14:35.148-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 57238"
time=2025-12-04T23:14:35.406-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 58080"
time=2025-12-04T23:14:36.344-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 56826"
time=2025-12-04T23:14:36.345-03:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\zeNk0\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --port 56827"
time=2025-12-04T23:14:36.769-03:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-dadf7e43-b580-d107-63fb-993daaa7f187 filtered_id="" library=CUDA compute=7.5 name=CUDA0 description="NVIDIA GeForce GTX 1660 SUPER" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:2b:00.0 type=discrete total="6.0 GiB" available="5.0 GiB"
time=2025-12-04T23:14:36.769-03:00 level=INFO source=routes.go:1618 msg="entering low vram mode" "total vram"="6.0 GiB" threshold="20.0 GiB"

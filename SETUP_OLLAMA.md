# üöÄ Configuraci√≥n de JARVIS con Ollama (IA Gratuita)

## ¬øPor qu√© Ollama?

**Ollama** te permite ejecutar modelos de IA de forma **100% local y gratuita** en tu computadora. No necesitas API keys ni pagas por uso.

### Ventajas de Ollama:
- ‚úÖ **Completamente GRATIS** - Sin l√≠mites de uso
- ‚úÖ **Privacidad total** - Los datos no salen de tu m√°quina
- ‚úÖ **Sin internet** - Funciona offline
- ‚úÖ **R√°pido** - Respuestas en segundos
- ‚úÖ **M√∫ltiples modelos** - Llama, Mistral, Phi, Gemma, etc.

---

## üì• Instalaci√≥n de Ollama

### Windows:
1. Descarga el instalador desde: https://ollama.com/download
2. Ejecuta `OllamaSetup.exe`
3. Ollama se ejecutar√° autom√°ticamente en segundo plano

### macOS:
```bash
brew install ollama
ollama serve
```

### Linux:
```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama serve
```

---

## ü§ñ Descargar Modelos

Una vez instalado Ollama, descarga un modelo de IA:

### Opci√≥n 1: Llama 3.2 (Recomendado - R√°pido y eficiente)
```bash
ollama pull llama3.2
```

### Opci√≥n 2: Mistral (Muy bueno para espa√±ol)
```bash
ollama pull mistral
```

### Opci√≥n 3: Phi-3 (El m√°s ligero)
```bash
ollama pull phi3
```

### Opci√≥n 4: Qwen2.5 (Excelente para espa√±ol)
```bash
ollama pull qwen2.5
```

**Tama√±os aproximados:**
- `llama3.2` - ~2GB
- `mistral` - ~4GB
- `phi3` - ~2.3GB
- `qwen2.5` - ~4.7GB

---

## ‚öôÔ∏è Configuraci√≥n de JARVIS

### 1. Instalar dependencias del proyecto
```bash
npm install
```

### 2. Inicializar la base de datos
```bash
npx prisma generate
npx prisma migrate dev
```

### 3. Configurar variables de entorno

Ya est√° configurado en `.env.local`:
```env
OLLAMA_HOST=http://localhost:11434
```

### 4. Inicializar datos del sistema

Ejecuta esto para crear comandos y configuraci√≥n inicial:

**Opci√≥n A: Desde el navegador**
```
POST http://localhost:3000/api/admin/seed
```

**Opci√≥n B: Con curl**
```bash
curl -X POST http://localhost:3000/api/admin/seed
```

---

## üß™ Probar la Configuraci√≥n

### 1. Verificar que Ollama est√© corriendo
```bash
ollama list
```

Deber√≠as ver tus modelos instalados.

### 2. Verificar el estado desde JARVIS

Visita:
```
GET http://localhost:3000/api/admin/ollama
```

Deber√≠as ver algo como:
```json
{
  "success": true,
  "available": true,
  "models": ["llama3.2:latest"],
  "activeModel": "llama3.2"
}
```

### 3. Iniciar el servidor de desarrollo
```bash
npm run dev
```

Visita: http://localhost:3000

---

## üì± Probar el Chatbot

1. **Env√≠a un mensaje de WhatsApp** a tu n√∫mero de bot (+56 9 6541 9765)

2. **JARVIS responder√°** usando Ollama de forma autom√°tica con:
   - Respuestas inteligentes contextuales
   - Memoria de conversaci√≥n
   - An√°lisis de intenci√≥n y sentiment
   - Soporte de comandos (`/ayuda`, `/info`, etc.)

---

## üéõÔ∏è Cambiar el Modelo de IA

Puedes cambiar el modelo en cualquier momento:

### Desde la base de datos:
```sql
UPDATE SystemConfig
SET value = 'mistral'
WHERE key = 'ai_model';
```

### O crear un endpoint de administraci√≥n:
```bash
POST /api/admin/config
{
  "key": "ai_model",
  "value": "mistral"
}
```

---

## üîç Modelos Recomendados por Uso

### Para Espa√±ol (mejor calidad):
- `qwen2.5` - Excelente comprensi√≥n del espa√±ol
- `mistral` - Muy bueno tambi√©n

### Para velocidad:
- `phi3` - El m√°s r√°pido
- `llama3.2` - Balance perfecto

### Para creatividad:
- `llama3.1:8b` - Muy creativo
- `mistral` - Respuestas elaboradas

---

## üêõ Soluci√≥n de Problemas

### Error: "No puedo conectarme al servidor de Ollama"

**Soluci√≥n:**
```bash
# Verifica si Ollama est√° corriendo
ollama list

# Si no est√° corriendo, in√≠cialo
ollama serve
```

### Error: "No hay modelos instalados"

**Soluci√≥n:**
```bash
ollama pull llama3.2
```

### Ollama est√° lento

**Soluci√≥n:**
1. Usa un modelo m√°s peque√±o (`phi3`)
2. Cierra otras aplicaciones pesadas
3. Considera usar GPU si la tienes disponible

---

## üìä Comparaci√≥n de Modelos

| Modelo | Tama√±o | Velocidad | Calidad Espa√±ol | RAM Necesaria |
|--------|--------|-----------|-----------------|---------------|
| phi3 | 2.3GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | 4GB |
| llama3.2 | 2GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | 4GB |
| mistral | 4GB | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 8GB |
| qwen2.5 | 4.7GB | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 8GB |

---

## üéâ ¬°Listo!

Ahora tienes un chatbot de WhatsApp con IA completamente **GRATIS** y **LOCAL**.

### Pr√≥ximos pasos:
- ‚úÖ Personaliza los comandos en la base de datos
- ‚úÖ Ajusta el sistema de prompts en `src/lib/ai.ts`
- ‚úÖ Crea nuevas funcionalidades
- ‚úÖ Despliega en producci√≥n

---

## üìö Recursos Adicionales

- **Ollama Docs**: https://ollama.com/docs
- **Modelos disponibles**: https://ollama.com/library
- **Ejemplos de prompts**: https://github.com/ollama/ollama/tree/main/examples

---

¬øNecesitas ayuda? Contacta al equipo de desarrollo.